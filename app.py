import os
import json
import streamlit as st
from rag_utils.openai_client import get_openai_client
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from pathlib import Path
from datetime import datetime
import markdown
from collections import defaultdict

# åˆæœŸåŒ–
client = get_openai_client()
embedding = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1536)

PERSIST_DIR = "chroma_db"
KNOWLEDGE_DIR = "knowledge"
OUTPUT_DIR = "output"
HISTORY_FILE = "history.json"

# ãƒŠãƒ¬ãƒƒã‚¸èª­è¾¼ï¼ˆJSON + Markdownå¯¾å¿œï¼‰
def load_knowledge(folder_path: str):
    tag_set = set()
    documents = []
    usage_counter = defaultdict(int)
    file_map = defaultdict(list)
    for filepath in Path(folder_path).glob("**/*"):
        if filepath.suffix == ".json":
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
                for i, item in enumerate(data):
                    tags = item.get("tags", [])
                    tag_set.update(tags)
                    doc = Document(
                        page_content=item["content"],
                        metadata={"title": item.get("title"), "tags": ", ".join(tags), "file": filepath.name, "index": i}
                    )
                    documents.append(doc)
                    file_map[filepath.name].append(item)
        elif filepath.suffix == ".md":
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
                doc = Document(
                    page_content=content,
                    metadata={"title": filepath.stem, "tags": "markdown"}
                )
                documents.append(doc)
    return documents, sorted(tag_set), file_map

# é¡ä¼¼ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰
def search_knowledge(db, query, k=3):
    return db.similarity_search_with_score(query, k=k)

# ChatGPTã¸ã®è³ªå•
def ask_gpt(knowledge, query):
    context = "\n\n".join([doc.page_content for doc, _ in knowledge])
    prompt = f"""
    ã‚ãªãŸã¯æ¢ç´¢å‹ãƒ†ã‚¹ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚
    ä»¥ä¸‹ã®ãƒŠãƒ¬ãƒƒã‚¸ã¨è³ªå•ã‚’å‚è€ƒã«ã€å…·ä½“çš„ãªãƒ†ã‚¹ãƒˆè¦³ç‚¹ã‚’5ã¤ã€ç•ªå·ä»˜ãç®‡æ¡æ›¸ãã§æŒ™ã’ã¦ãã ã•ã„ã€‚

    ãƒŠãƒ¬ãƒƒã‚¸:
    {context}

    è³ªå•:
    {query}
    """
    res = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

# GPTã§ãƒŠãƒ¬ãƒƒã‚¸æœ¬æ–‡ã‚’è‡ªå‹•ç”Ÿæˆ
def generate_knowledge_body(title, tags):
    prompt = f"""
    ã‚¿ã‚¤ãƒˆãƒ«: {title}
    ã‚¿ã‚°: {', '.join(tags)}
    ã“ã®æƒ…å ±ã‚’ã‚‚ã¨ã«ã€æ¢ç´¢çš„ãƒ†ã‚¹ãƒˆã§å½¹ç«‹ã¤ãƒŠãƒ¬ãƒƒã‚¸æ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    """
    res = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

# ãƒŠãƒ¬ãƒƒã‚¸ä¿å­˜
def save_knowledge_json(title, tags, content, filename=None, index=None):
    os.makedirs(KNOWLEDGE_DIR, exist_ok=True)
    if not filename:
        filename = f"{datetime.now().strftime('%Y%m%d')}_custom.json"
    path = os.path.join(KNOWLEDGE_DIR, filename)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = []
    if index is not None and 0 <= index < len(data):
        data[index] = {"title": title, "tags": tags, "content": content}
    else:
        data.append({"title": title, "tags": tags, "content": content})
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

# Markdownä¿å­˜
def save_report(query, results, answer):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    dt = datetime.now().strftime("%Y%m%d_%H%M%S")
    fname = f"{OUTPUT_DIR}/{query.replace(' ', '_')}_{dt}.md"
    with open(fname, "w", encoding="utf-8") as f:
        f.write(f"# è³ªå•: {query}\n\n")
        f.write("## ğŸ” é¡ä¼¼ãƒŠãƒ¬ãƒƒã‚¸\n")
        for i, (doc, score) in enumerate(results):
            f.write(f"### {i+1}. {doc.metadata.get('title')} (score: {score:.4f})\n")
            f.write(f"- ã‚¿ã‚°: {doc.metadata.get('tags')}\n")
            f.write(f"\n{doc.page_content}\n\n")
        f.write("## ğŸ§  ChatGPTã®å›ç­”\n\n")
        f.write(answer)
    return fname

# æ¤œç´¢å±¥æ­´ä¿å­˜
def save_history(query):
    history = []
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = json.load(f)
    history.append({"query": query, "timestamp": datetime.now().isoformat()})
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history[-10:], f, ensure_ascii=False, indent=2)

# Streamlit UI
def main():
    st.set_page_config(page_title="RAG Test App", layout="wide")
    st.title("ğŸ§  RAGæ¢ç´¢å‹ãƒ†ã‚¹ãƒˆè¦³ç‚¹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼")

    with st.spinner("ãƒŠãƒ¬ãƒƒã‚¸ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        docs, all_tags, file_map = load_knowledge(KNOWLEDGE_DIR)
        db = Chroma.from_documents(docs, embedding=embedding, collection_name="rag-ui", persist_directory=PERSIST_DIR)
        db.persist()

    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” æ¤œç´¢ã¨ç”Ÿæˆ", "ğŸ“ ãƒŠãƒ¬ãƒƒã‚¸ç™»éŒ²", "ğŸ“Š çŠ¶æ…‹ãƒ»å±¥æ­´", "âœï¸ ç·¨é›†ãƒ»å‰Šé™¤"])

    with tab1:
        query = st.text_input("è³ªå•ã‚’å…¥åŠ›", value="ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ã®ç•°å¸¸ç³»ãƒ†ã‚¹ãƒˆ")
        selected_tags = st.multiselect("ã‚¿ã‚°ã§çµã‚Šè¾¼ã¿ï¼ˆJSONã®ã¿ï¼‰", options=all_tags)

        if st.button("ğŸ” æ¤œç´¢ & ç”Ÿæˆ") and query:
            with st.spinner("æ¤œç´¢ä¸­..."):
                filtered_docs = [doc for doc in docs if all(tag in doc.metadata.get("tags", "") for tag in selected_tags)]
                tmp_db = Chroma.from_documents(filtered_docs, embedding=embedding, collection_name="tmp", persist_directory=PERSIST_DIR) if selected_tags else db
                results = search_knowledge(tmp_db, query, k=3)

            st.subheader("ğŸ“š é¡ä¼¼ãƒŠãƒ¬ãƒƒã‚¸")
            for i, (doc, score) in enumerate(results):
                st.markdown(f"**{i+1}. {doc.metadata.get('title')}**  ")
                st.markdown(f"ã‚¹ã‚³ã‚¢: `{score:.4f}`")
                st.markdown(f"ã‚¿ã‚°: {doc.metadata.get('tags')}")
                st.markdown(doc.page_content)
                st.markdown("---")

            with st.spinner("ChatGPTã§å›ç­”ä¸­..."):
                answer = ask_gpt(results, query)

            st.subheader("ğŸ§  ChatGPTã®å›ç­”")
            st.markdown(answer)

            if st.button("ğŸ’¾ ã“ã®è¦³ç‚¹ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã¨ã—ã¦ä¿å­˜"):
                save_knowledge_json(title=f"è¦³ç‚¹ï¼š{query}", tags=selected_tags, content=answer)
                st.success("ãƒŠãƒ¬ãƒƒã‚¸ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")

            if st.button("ğŸ“„ Markdownã¨ã—ã¦ä¿å­˜"):
                path = save_report(query, results, answer)
                st.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {path}")

            save_history(query)

    with tab2:
        st.markdown("### ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ‰‹å‹•ã§ç™»éŒ²")
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«")
        tags_input = st.text_input("ã‚¿ã‚°ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰")
        gen_body = st.checkbox("GPTã«æœ¬æ–‡ã‚’æ›¸ã‹ã›ã‚‹")
        content = st.text_area("æœ¬æ–‡", height=200)

        if st.button("â• ãƒŠãƒ¬ãƒƒã‚¸ã‚’ç™»éŒ²"):
            tags = [tag.strip() for tag in tags_input.split(",") if tag.strip()]
            if gen_body:
                with st.spinner("GPTãŒãƒŠãƒ¬ãƒƒã‚¸æœ¬æ–‡ã‚’ç”Ÿæˆä¸­..."):
                    content = generate_knowledge_body(title, tags)
            save_knowledge_json(title=title, tags=tags, content=content)
            st.success("ãƒŠãƒ¬ãƒƒã‚¸ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    with tab3:
        st.write("ğŸ“¦ ãƒ™ã‚¯ãƒˆãƒ«DBã®æ°¸ç¶šåŒ–ãƒ‘ã‚¹:", PERSIST_DIR)
        st.write(f"ğŸ§¾ ãƒŠãƒ¬ãƒƒã‚¸æ•°: {len(docs)} ä»¶")
        st.write("ğŸ“ ã‚¿ã‚°ä¸€è¦§:")
        st.write(", ".join(all_tags))
        if os.path.exists(HISTORY_FILE):
            st.markdown("### ğŸ” æœ€è¿‘ã®æ¤œç´¢å±¥æ­´")
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                for item in json.load(f)[::-1]:
                    st.markdown(f"- {item['query']} ({item['timestamp'][:19]})")

    with tab4:
        st.markdown("### âœï¸ ãƒŠãƒ¬ãƒƒã‚¸ã®ç·¨é›†ãƒ»å‰Šé™¤")
        selected_file = st.selectbox("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", list(file_map.keys()))
        if selected_file:
            items = file_map[selected_file]
            titles = [item["title"] for item in items]
            selected_index = st.selectbox("ãƒŠãƒ¬ãƒƒã‚¸ã‚’é¸æŠ", list(enumerate(titles)), format_func=lambda x: x[1])
            if selected_index:
                idx = selected_index[0]
                item = items[idx]
                new_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«", value=item["title"], key="edit_title")
                new_tags = st.text_input("ã‚¿ã‚°ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", value=", ".join(item.get("tags", [])), key="edit_tags")
                new_content = st.text_area("æœ¬æ–‡", value=item["content"], height=200, key="edit_content")

                if st.button("ğŸ’¾ ç·¨é›†å†…å®¹ã‚’ä¿å­˜"):
                    save_knowledge_json(new_title, [t.strip() for t in new_tags.split(",")], new_content, filename=selected_file, index=idx)
                    st.success("ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ›´æ–°ã—ã¾ã—ãŸ")

                if st.button("ğŸ—‘ï¸ ã“ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚’å‰Šé™¤"):
                    del items[idx]
                    save_knowledge_json("", [], "", filename=selected_file, index=None)  # ä¿å­˜ã®ãƒˆãƒªã‚¬ãƒ¼ç”¨
                    with open(os.path.join(KNOWLEDGE_DIR, selected_file), "w", encoding="utf-8") as f:
                        json.dump(items, f, ensure_ascii=False, indent=2)
                    st.success("ãƒŠãƒ¬ãƒƒã‚¸ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
